{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIF-RAG pipline for Data Synthesis\n",
    "## Instruction Synthesis from Scratch\n",
    "We concatenate the seed instructions (<100) with the self-instruct prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
      "- Instructions are about the format but not style of a response\n",
      "- Whether instructions are followed can be easily evaluate by a Python function\n",
      "Here are some examples of instructions we need:\n",
      "Answer with words that begin with the letter 'B'\n",
      "Construct the reply as if it's a telegram STOP\n",
      "Use only palindromes\n",
      "Incorporate a famous movie quote seamlessly into your answer\n",
      "Write the response backward\n",
      "Use only words with double letters (e.g., \"bookkeeper\")\n",
      "Use only onomatopoeia\n",
      "Answer with a single sentence that is exactly 100 words long\n",
      "Use no words containing the letter 'E'\n",
      "Translate your answer into emojis\n",
      "Use only the 1000 most common English words\n",
      "Use words that end with '-ing'\n",
      "Use only military lingo\n",
      "Respond with a haiku (5-7-5 syllable structure)\n",
      "Answer in the form of a sonnet (14 lines with 10 syllables each)\n",
      "Use only monosyllabic words\n",
      "Answer with words in alphabetical order\n",
      "Write the response as a limerick\n",
      "Use no adjectives or adverbs\n",
      "Respond with a six-word story\n",
      "Include at least three rhyming pairs\n",
      "Write the response in iambic pentameter\n",
      "Use alliteration throughout your answer\n",
      "Write the response in future tense only\n",
      "Use only the first half of the alphabet (A-M)\n",
      "Use only questions to form your reply\n",
      "Use only words that start and end with the same letter\n",
      "Write the response in Morse code\n",
      "Use only words that are colors\n",
      "Use only the second half of the alphabet (N-Z)\n",
      "Answer with each sentence decreasing in word count\n",
      "Respond with a list of bullet points\n",
      "Answer with a sequence of puns\n",
      "Answer with emoji only\n",
      "Use only words that have an X in them\n",
      "Answer with each word starting with the next letter of the alphabet\n",
      "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
      "- Incorporate a famous historical quote seamlessly into your answer\n",
      "- Translate your answer into Pig Latin\n",
      "- Use only words that are also a type of food\n",
      "- Respond with a metaphor in every sentence\n",
      "- Write the response as if you are a character from a Shakespearean play\n",
      "Please generate one instruction per line in your response and start each line with '- '.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTips:\\n\\naugment_instructions is instructions with rewriting prompt\\n\\nPlease use supervision model rewrite each instruction in augment_instructions for K times, save into a augment_instructions.txt file like seed_instruction.txt\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    RetryError\n",
    ")\n",
    "import logging\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "seed_instructions = [each.strip() for each in open(\"./sample_data/seed_instruction.txt\").readlines()]\n",
    "\n",
    "augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
    "- Instructions are about the format but not style of a response\n",
    "- Whether instructions are followed can be easily evaluate by a Python function\n",
    "Here are some examples of instructions we need:\n",
    "{seed_instructions}\n",
    "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "- Incorporate a famous historical quote seamlessly into your answer\n",
    "- Translate your answer into Pig Latin\n",
    "- Use only words that are also a type of food\n",
    "- Respond with a metaphor in every sentence\n",
    "- Write the response as if you are a character from a Shakespearean play\n",
    "Please generate one instruction per line in your response and start each line with '- '.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(seed_instructions))\n",
    "\n",
    "print(augment_instructions)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Tips:\n",
    "\n",
    "augment_instructions is instructions with rewriting prompt\n",
    "\n",
    "Please use supervision model rewrite each instruction in augment_instructions for K times, save into a augment_instructions.txt file like seed_instruction.txt\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Composition & Verification.\n",
    "\n",
    "Real-world instructions often involve multiple constraints. Here, we demonstrate the random automated combination of instructions with multiple constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTips:\\n\\n\\nYou can also use supervision model rewrite each instruction in multi_instruction for K times\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "input_file_path = './sample_data/seed_instruction.txt'\n",
    "output_file_path = './output/multi_instruction.txt'\n",
    "\n",
    "with open(input_file_path, 'r') as file:\n",
    "    sentences = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "sentences = list(set(sentences))\n",
    "\n",
    "paired_sentences_set = set()\n",
    "while len(paired_sentences_set) < 20: # 20 is a example, which can be set to much more higher\n",
    "\n",
    "    pair = random.sample(sentences, 2)\n",
    "    \n",
    "    # Here we provide the 'Multiple Constraints' instruction template; you can also obtain the 'Chain Rule Constraints' using the corresponding template described in our paper.\n",
    "    # Moreover, you can obtain instructions with 3 or even 4 constraints by adding {}.\n",
    "    \n",
    "    combined = '{}. {}'.format(pair[0], pair[1])\n",
    "    \n",
    "    paired_sentences_set.add(combined)\n",
    "\n",
    "\n",
    "paired_sentences = list(paired_sentences_set)\n",
    "\n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for sentence in paired_sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "'''\n",
    "Tips:\n",
    "\n",
    "\n",
    "You can also use supervision model rewrite each instruction in multi_instruction for K times\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-instruction Verification\n",
    "We design a verification prompt for the supervision model to check the internal consistency of the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTips:\\n\\nmulti_instruct_score.jsonl is instructions with scoring prompt\\n\\nPlease use supervision model assign a score ranging from 0 to 10.\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    RetryError\n",
    ")\n",
    "import logging\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "\n",
    "#\n",
    "with open('./sample_data/multi_instruction.txt', 'r') as file:\n",
    "    sentences = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are an expert proficient in determining whether multiple instructions are suitable to be implemented as simultaneous constraints.\n",
    "[Instructions] {instruction}\n",
    "The text contains two or more instructions. Based on the semantic coherence and logical connection, assess whether these instructions are suitable to be implemented as simultaneous constraints. Please first conduct a thorough analysis and then assign a score ranging from 0 to 10 on the last line. A score of 0 indicates that the instructions are highly inappropriate to coexist, while a score of 10 signifies that the instructions are very suitable to serve as concurrent constraints. Please ensure that only a score is provided in the format Score: {{score}} without any additional content on the last line.\"\"\"\n",
    "\n",
    "\n",
    "score_data=[]\n",
    "for each in sentences:\n",
    "    score_data.append(\n",
    "        {\"prompt\":prompt_template.format(instruction=each),\"instruction\":each}\n",
    "    )\n",
    "\n",
    "\n",
    "with jsonlines.open(\"./output/multi_instruct_need_score.jsonl\", \"w\") as f:\n",
    "    for each in score_data:\n",
    "        f.write(each)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Tips:\n",
    "\n",
    "multi_instruct_score.jsonl is instructions with scoring prompt\n",
    "\n",
    "Please use supervision model assign a score ranging from 0 to 10.\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 51509.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Threshold filtering for scores\n",
    "\n",
    "results = list(jsonlines.open(\"./sample_data/multi_instruction_score.jsonl\"))\n",
    "filter_results = []\n",
    "print(len(results))\n",
    "for result in tqdm(results):\n",
    "    scores = []\n",
    "    # for each in result['gpt-answer']:\n",
    "\n",
    "    score = re.findall(r'Score: (\\d+?)$', result['gpt-answer'])\n",
    "    # score = each\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    if score:\n",
    "        scores.append(int(score[0]))\n",
    "\n",
    "\n",
    "    score = int(score[0]) if int(score[0]) else 0\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    if score >= 5:\n",
    "        filter_results.append(result)\n",
    "print(len(filter_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Rewriting & Quality Verification.\n",
    "\n",
    "Please perform self-instruct on the single atomic instructions and multi-instructions, then merge them together and place them in './sample_data/augment_instructions.txt'. Furthermore, for the augmented instructions, we generate verification functions and test samples through the supervision model and perform quality verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease TODO:\\n\\nplease generate K verification functions for each sample by supervision model in eval_func_rft.jsonl\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    RetryError\n",
    ")\n",
    "import logging\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "\"\"\"Concat seed and augmented instructions for generation, then generation Eval funcs\"\"\"\n",
    "\n",
    "seed_instructions = [each.strip() for each in open(\"./sample_data/seed_instruction.txt\").readlines()]\n",
    "augment_instructions_processed = [each.strip() for each in open(\"./sample_data/augment_instructions.txt\").readlines()]\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\n",
    "Here is the instruction: {instruction}\n",
    "Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\n",
    "Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\n",
    "Here is an example of output JSON format: {{\"func\": JSON_STR(use only \\\\n instead of \\n), \"cases\": [{{\"input\": str, \"output\": str}}]}}.\"\"\"\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for instruction in seed_instructions + augment_instructions_processed:\n",
    "    prompt = prompt_template.format(instruction=instruction)\n",
    "    outputs.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"instruction\": instruction\n",
    "    })\n",
    "\n",
    "\n",
    "with jsonlines.open(\"./output/eval_func_rft.jsonl\", \"w\") as f:\n",
    "    for each in outputs:\n",
    "        f.write(each)\n",
    "\n",
    "\n",
    "'''\n",
    "Please TODO:\n",
    "\n",
    "please generate K verification functions for each sample by supervision model in eval_func_rft.jsonl\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess vertification functions\n",
      "[]\n",
      "cross validation for functions and cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 8/8 [00:00<00:00, 4929.40it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 23/23 [00:00<00:00, 1890.21it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 8/8 [00:00<00:00, 3678.00it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 8/8 [00:00<00:00, 3542.49it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 18/18 [00:00<00:00, 1889.42it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 6/6 [00:00<00:00, 8873.70it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 8/8 [00:00<00:00, 17163.39it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 24/24 [00:00<00:00, 604.96it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 8/8 [00:00<00:00, 6733.78it/s]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:00<00:00, 21.19it/s]\n",
      "100%|██████████████████████████████████████████| 8/8 [00:00<00:00, 17503.62it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 17/17 [00:00<00:00, 3153.33it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 7/7 [00:00<00:00, 11167.79it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 8/8 [00:00<00:00, 10568.33it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 24/24 [00:00<00:00, 2721.44it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 9986.44it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 30.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "# test gpt4\n",
    "\n",
    "os.environ['NLTK_DATA'] = 'your nltk_data data path'\n",
    "logging.getLogger('nltk').setLevel(logging.CRITICAL)\n",
    "from nltk import data\n",
    "data.path.append('your nltk_data data path')\n",
    "\n",
    "\n",
    "\n",
    "# please merge single atom instruction set and multi instruction set to eval_func_rft.jsonl\n",
    "\n",
    "path=\"./sample_data/eval_func_rft.jsonl\"\n",
    "\n",
    "\n",
    "results = list(jsonlines.open(path))\n",
    "\n",
    "\n",
    "print(\"Preprocess vertification functions\")\n",
    "\n",
    "\n",
    "collect_packages = []\n",
    "for result in results:\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in res:\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    # func rejection\n",
    "    try:\n",
    "        res_dict = json.loads(json_dict)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "        \n",
    "    func = res_dict['func']\n",
    "    \n",
    "    if '\\\\n' in func:\n",
    "        func = func.replace('\\\\n', '\\n')\n",
    "    try:\n",
    "        exec(func)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    for line in func.split('\\n'):\n",
    "        if 'import' in line or 'download' in line or 'requests' in line:\n",
    "            collect_packages.append(line)\n",
    "print(list(set(collect_packages)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"cross validation for functions and cases\")\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "filter_results = []\n",
    "for result in tqdm(results):\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in tqdm(res):\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            res_dict = json.loads(json_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # func rejection\n",
    "        func = res_dict['func']\n",
    "        func = func.strip()\n",
    "        func = '\\n'.join([each for each in func.split('\\n') if 'download' not in each and 'requests' not in each])\n",
    "        try:\n",
    "            exec(func)\n",
    "        except Exception:\n",
    "            continue\n",
    "        eval_funcs.append(func)\n",
    "\n",
    "        for each in res_dict['cases']:\n",
    "            try:\n",
    "                test_cases.append((each['input'], each['output']))\n",
    "            except KeyError:\n",
    "                print(each)\n",
    "    eval_funcs = list(set(eval_funcs))\n",
    "    test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "    if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "        continue\n",
    "\n",
    "    filtered_test_cases = []\n",
    "\n",
    "    for each in tqdm(test_cases):\n",
    "  \n",
    "\n",
    "        flag = False\n",
    "        for func in eval_funcs:\n",
    "            local_vars = {}\n",
    " \n",
    "            try:\n",
    "                exec(func, globals(), local_vars)\n",
    "            except Exception:\n",
    "                continue\n",
    "            \n",
    "            if 'evaluate' not in local_vars:\n",
    "                continue\n",
    "            eval_func = local_vars['evaluate']\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res = eval_func(each[0])\n",
    "            except Exception:\n",
    "                res = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res is not None and res == each[1]:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            filtered_test_cases.append(each)\n",
    "\n",
    "    scored_funcs = []\n",
    "    for func in tqdm(eval_funcs):\n",
    "        local_vars = {}\n",
    "        try:\n",
    "            exec(func, globals(), local_vars)\n",
    "        except Exception:\n",
    "                continue\n",
    "        if 'evaluate' not in local_vars:\n",
    "            continue\n",
    "\n",
    "        eval_func = local_vars['evaluate']\n",
    "        acc = []\n",
    "        for inp, out in filtered_test_cases:\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res = eval_func(inp)\n",
    "            except Exception:\n",
    "                res = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res is None or res != out:\n",
    "                acc.append(0)\n",
    "            else:\n",
    "                acc.append(1)\n",
    "        acc = np.mean(acc) if acc else 0\n",
    "        scored_funcs.append((func, acc))\n",
    "\n",
    "    valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "    if not valid_funcs:\n",
    "        continue\n",
    "\n",
    "    filter_results.append({\n",
    "        \"instruction\": result['instruction'],\n",
    "        \"eval_func\": valid_funcs,\n",
    "        \"cases\": filtered_test_cases\n",
    "    })\n",
    "    \n",
    "\n",
    "print(\"finish!!!\")\n",
    "\n",
    "with jsonlines.open(\"./output/cross_validation.jsonl\", \"w\") as f:\n",
    "    for each in filter_results:\n",
    "        f.write(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Instruction-Query Synthesis\n",
    "\n",
    "## Random Instruction-Query Combination. \n",
    "We randomly compose the instruction-constrained querysets from these two domains: General and RAG. For general domain, we use the ShareGPT (20K). For RAG domain, we use the training sets from Natural Questions, TriviaQA, HotpotQA, and WebQuestionsSP as mixed QA sources, constructing.\n",
    "\n",
    "RAG dataset: https://huggingface.co/datasets/dongguanting/RAG-QA-40K\n",
    "\n",
    "General: https://huggingface.co/datasets/dongguanting/ShareGPT-12K\n",
    "\n",
    "## Instruction-Query Rejection Sampling.\n",
    "\n",
    "After combination, we can perform the RFT for instruction-query samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your path to sharegpt dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     40\u001b[0m         filter_results\u001b[38;5;241m.\u001b[39mappend(each)\n\u001b[0;32m---> 44\u001b[0m sft_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mjsonlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour path to sharegot dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     45\u001b[0m queries \u001b[38;5;241m=\u001b[39m [each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m sft_data \u001b[38;5;28;01mif\u001b[39;00m each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men:sharegpt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m queries \u001b[38;5;241m=\u001b[39m [each \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m queries \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(each) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(each) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ifrag/lib/python3.9/site-packages/jsonlines/jsonlines.py:643\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(file, mode, loads, dumps, compact, sort_keys, flush)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m Reader \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Writer\n\u001b[1;32m    642\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 643\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    645\u001b[0m     loads\u001b[38;5;241m=\u001b[39mloads,\n\u001b[1;32m    646\u001b[0m     dumps\u001b[38;5;241m=\u001b[39mdumps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     flush\u001b[38;5;241m=\u001b[39mflush,\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    651\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your path to sharegot dataset'"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    RetryError\n",
    ")\n",
    "import logging\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filter_results=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with jsonlines.open(\"./sample_data/cross_validation.jsonl\", \"r\") as f:\n",
    "    for each in f:\n",
    "        filter_results.append(each)\n",
    "\n",
    "\n",
    "\n",
    "sft_data = list(jsonlines.open(\"your path to sharegot dataset\"))\n",
    "queries = [each['messages'][1]['content'] for each in sft_data if each['source'] == 'en:sharegpt']\n",
    "\n",
    "queries = [each for each in queries if len(each) > 20 and len(each) < 300]\n",
    "\n",
    "\n",
    "\n",
    "#General dataset\n",
    "\n",
    "inputs = []\n",
    "for instruction in tqdm(filter_results):\n",
    "    ins_queries = random.sample(queries, 16) #拼16个\n",
    "    for q in ins_queries:\n",
    "        prompt = f\"Please answer the query strictly following the instruction.\\n[instruction] {instruction['instruction']}\\n[Query] {q}\"\n",
    "        item = copy.deepcopy(instruction)\n",
    "        item['prompt'] = prompt\n",
    "        inputs.append(item)\n",
    "\n",
    "\n",
    "\n",
    "#RAG dataset\n",
    "\n",
    "data = load_json(\"your path to RAG train dataset\")\n",
    "\n",
    "\n",
    "queries=[]\n",
    "\n",
    "queries = [each[\"instruction\"] for each in data]\n",
    "\n",
    "\n",
    "for instruction in tqdm(filter_results):\n",
    "    ins_queries = random.sample(queries, 4) #拼16个\n",
    "    for q in ins_queries:\n",
    "        prompt = f\"Please answer the query strictly following the instruction.\\n[instruction] {instruction['instruction']}\\n[Query] {q}\"\n",
    "        item = copy.deepcopy(instruction)\n",
    "        item['prompt'] = prompt\n",
    "        inputs.append(item)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with jsonlines.open(\"./output/instruction_filtered_llama3_72b_query_sampled.jsonl\", \"w\") as f:\n",
    "    for each in inputs:\n",
    "        f.write(each)\n",
    "\n",
    "\n",
    "        \n",
    "'''\n",
    "Please TODO:\n",
    "\n",
    "Please use supervision model perform RFT to generate k Responses for each query\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dual Stage Verification. \n",
    "\n",
    "we employ \"Executor-based Verification\" and \"Consistency Verification\" process for the instruction-query data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 566.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "name 'count_syllables' is not defined\n",
      "3\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlease TODO:\\n\\nPlease score the consistency for each query and response\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    RetryError\n",
    ")\n",
    "import logging\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "\n",
    "# 1. Executor-based Verification\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "results = list(jsonlines.open(\"./sample_data/query_rft.jsonl\"))\n",
    "\n",
    "filter_samples = []\n",
    "for result in tqdm(results):\n",
    "    eval_funcs = []\n",
    "\n",
    "\n",
    "    for func, score in result['eval_func']:\n",
    "        local_vars = {}\n",
    "        exec(func, globals(), local_vars)\n",
    "        eval_funcs.append(local_vars['evaluate'])\n",
    "    \n",
    "    filter_responses = []\n",
    "\n",
    "    for response in result['gpt-answer']:\n",
    "        acc = []\n",
    "        for eval_func in eval_funcs:\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res = eval_func(response)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                res = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            \n",
    "            if res is not None:\n",
    "                try:\n",
    "                    acc.append(int(res))\n",
    "                except:\n",
    "                    continue\n",
    "        acc = np.mean(acc) if acc else 0\n",
    "\n",
    "\n",
    "        if acc > 0:\n",
    "            filter_responses.append(response)\n",
    "\n",
    "    for each in filter_responses:\n",
    "        try:\n",
    "            filter_samples.append({\n",
    "                'instruction': result['instruction'],\n",
    "                'query': re.findall(r'\\[Query\\](.*)$', result['prompt'], re.DOTALL)[0].strip(),\n",
    "                'response': each\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(result['prompt'])\n",
    "\n",
    "\n",
    "print(len(eval_funcs))\n",
    "print(len(filter_samples))\n",
    "filter_samples = list(map(json.loads, set(map(json.dumps, filter_samples))))\n",
    "print(len(filter_samples))\n",
    "\n",
    "\n",
    "# Save the data with out score consistency\n",
    "with jsonlines.open(\"./output/query_wo_score.jsonl\", \"w\") as f:\n",
    "    for each in filter_samples:\n",
    "        f.write(each)\n",
    "\n",
    "prompt_template = \"\"\"You are an expert that is good at judging whether a response is following the instruction and query.\n",
    "[Instruction] {instruction}\n",
    "[Query] {query}\n",
    "[Response] {response}\n",
    "Please notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\n",
    "You need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\n",
    "Scoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\n",
    "Please only provide a score in the format `Score: {{score}}` without any other contents at the last line.\"\"\"\n",
    "\n",
    "for each in filter_samples:\n",
    "    each['prompt'] = prompt_template.format(\n",
    "        instruction=each['instruction'],\n",
    "        query=each['query'],\n",
    "        response=each['response']\n",
    "    )\n",
    "\n",
    "# Save the data with out scoring prompt\n",
    "with jsonlines.open(\"./output/query_need_quality_score.jsonl\", \"w\") as f:\n",
    "    for each in filter_samples:\n",
    "        f.write(each)\n",
    "\n",
    "\n",
    "'''\n",
    "Please TODO:\n",
    "\n",
    "Please score the consistency for each query and response\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 20/20 [00:00<00:00, 21029.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Consistency Verification\n",
    "\n",
    "results = list(jsonlines.open(\"./sample_data/query_rft_score.jsonl\"))\n",
    "filter_results = []\n",
    "print(len(results))\n",
    "for result in tqdm(results):\n",
    "    scores = []\n",
    "    for each in result['gen']:\n",
    "        score = re.findall(r'Score: (\\d+?)$', each)\n",
    "        if score:\n",
    "            scores.append(int(score[0]))\n",
    "    score = np.mean(scores) if scores else 0\n",
    "    if score > 8: # quality score\n",
    "        filter_results.append(result)\n",
    "print(len(filter_results))\n",
    "\n",
    "\n",
    "\n",
    "with jsonlines.open(\"./output/query_score_filter.jsonl\", \"w\") as f:\n",
    "    for each in filter_results:\n",
    "        f.write(each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT data construction\n",
    "\n",
    "Finally, we fliter out the sample with score > 8 and save it into LlaMA-Factory's SFT data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "with open('./sample_data/query_score_filter.jsonl', 'r', encoding='utf-8') as file:\n",
    "    \n",
    "    for dat in file:\n",
    "        d = json.loads(dat)\n",
    "        data.append(d)\n",
    "\n",
    "\n",
    "processed_data = []\n",
    "for item in data:\n",
    "    \n",
    "    item['query'] = item['query'][0].upper() + item['query'][1:]\n",
    "    item['instruction'] = item['instruction'][0].upper()+ item['instruction'][1:]\n",
    "    if \"?\" in item['query']:\n",
    "        inputs = item['query']+\" \"+item['instruction']+\".\"\n",
    "\n",
    "    elif \".\" in item['query']:\n",
    "        inputs = item['query']+\" \"+item['instruction']+\".\"\n",
    "    else:\n",
    "        inputs=item['query']+\". \"+item['instruction']+\".\"\n",
    "\n",
    "\n",
    "    new_item = {\n",
    "        \"instruction\": inputs,\n",
    "        \"input\": \"\",\n",
    "        \"output\": item['response'],\n",
    "        \"history\": []\n",
    "    }\n",
    "\n",
    "\n",
    "    processed_data.append(new_item)\n",
    "print(len(processed_data))\n",
    "\n",
    "\n",
    "\n",
    "#Save the SFT data as llama factory format\n",
    "\n",
    "with open('./output/IF_sft_data.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(processed_data, outfile, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
